# Don't try to use this in the crawler.
# It will break things.

# Number of threads to crawl with.
thread-count = <Int>

# Whether to overwrite the current save files or not.
overwrite = <True/False>

# Whether to stop the crawler after hitting an unknown error.
raise-errors = <True/False>

# Whether to save pages after crawling.
save-pages = <True/False>

# Whether to scrape and process words.
save-words = <True/False>

# Whether to zip saved pages when autosaving.
zip-files = <True/False>

# Whether to get documents larger than 500 MB
override-size = <True/False>

# The domain within which to restrict crawling.
domain = example.com

# Whether to respect sites' robots.txt or not
respect-robots = <True/False>

# Location of the TODO save file.
todo-file = /path/to/todo_file.txt

# Location of the done save file.
done-file = /path/to/done_file.txt

# Location of the word save file.
word-file = /path/to/word_file.txt

# Number of queried links after which to autosave.
save-count = <Int>

# Which browser the crawler should headers for. (spidy, Chrome, IE, Edge)
header-preset = <Header>
# Or if you want to use custom headers:
#header = {'<Header Name>': '<Value>', '<Header2>': '<Value2>'}

# Amount of errors allowed to happen before automatic shutdown.
max-new-errors = <Int>
max-known-errors = <Int>
max-http-errors = <Int>
max-new-mimes = <Int>
